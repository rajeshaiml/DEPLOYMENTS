# -*- coding: utf-8 -*-
"""SMS_Spam_Detection_V1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YIACOE8GQyAUcZJCGEejOeH_lcRC6nnb

# SMS SPAM Detection Coding Test
"""

from datetime import datetime
start_datetime = datetime.now()
print("Start time of the execution: ", start_datetime)

"""## Import Libraries"""

import pandas as pd
import re
import nltk
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, accuracy_score
import pickle

nltk.download('stopwords')

nltk.download('wordnet')

from nltk.corpus import stopwords
#from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer

"""## Load the data"""
data = pd.read_csv('spam.csv', encoding='latin-1')

#Got below error without encoding to latin-1, hence we need to encode the file before loading
#UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte
#https://stackoverflow.com/questions/5552555/unicodedecodeerror-invalid-continuation-byte

"""## Data analysis"""
"""Looks unnamed columns doesn't give much information, hence drop these columns"""
data.drop(labels=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)

#Change the colums names
data=data.rename(columns={"v1": "labels", "v2": "text"})


"""## Data Preprocessing"""
#ps = PorterStemmer()
lemmatizer = WordNetLemmatizer()
corpus = []
for i in range(0, len(data)):
    review = re.sub('[^a-zA-Z]', ' ', data['text'][i])
    review = review.lower()
    review = review.split()
    
    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)
    #data['processed_text'][i] = review

data['processed_text'] = corpus

data=data.drop('text',axis=1)

X = data['processed_text']
y=pd.get_dummies(data['labels'])

y=y['ham'].values

"""## Data Splitting"""

X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=0)

# Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=3500)
#X = cv.fit_transform(corpus).toarray()
X_train_bow = cv.fit_transform(X_train['processed_text']).toarray()
X_test_bow = cv.transform(X_test['processed_text']).toarray()

#Save the countvectorizer model
pickle.dump(cv, open('countvectorizer.pkl','wb'))


"""## Model using BOW"""

mul_nb_bow = MultinomialNB().fit(X_train_bow, y_train)

y_pred_nb_bow=mul_nb_bow.predict(X_test_bow)

confusion_matrix(y_pred_nb_bow, y_test)

accuracy_nb_bow=accuracy_score(y_pred_nb_bow, y_test)

'''
#TFIDF
from sklearn.feature_extraction.text import TfidfVectorizer
Vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,2))
X_train_tfidf = Vectorizer.fit_transform(X_train['processed_text'])
X_test_tfidf = Vectorizer.transform(X_test['processed_text'])

#Save the countvectorizer model
pickle.dump(Vectorizer, open('TFIDFVectorizer.pkl','wb'))


"""## Model using TFIDF"""

mul_nb_tfidf = MultinomialNB().fit(X_train_tfidf, y_train)

y_pred_nb_tfidf=mul_nb_tfidf.predict(X_test_tfidf)

confusion_matrix(y_pred_nb_tfidf, y_test)

accuracy_nb_tfidf=accuracy_score(y_pred_nb_tfidf, y_test)
'''
"""As we got 98.57% accuracy using Multinomial Naive Bayes model and keeping the time constraint in mind, not comparing with the deep learning models (CNN, LSTM, BERT etc)
"""

# Saving model to disk
import pickle
pickle.dump(mul_nb_bow, open('model.pkl','wb'))

'''
# Loading model to compare the results
model_load = pickle.load(open('model.pkl','rb'))
print(model_load.predict(X_test))
y_pred_pkl=model_load.predict(X_test)
'''

'''
There are multiple options to choose the best model. 
I prefer to run pycaret as it gives basic idea on how the ML models are performing on the given dataset. 
There are mulitple options available in pycaret, I just did simple analysis. 
We can also run few set of models based on the understanding of the data. 
Usually Naive Bayes/probablistic approach works better OR gives better accuracies for the 
text classification problems and it is proved as per my experience and the community.
Hence used MultinomianalNB for the better results. 
'''

Also, Deep Learning (CNN, LSTMs, BERT etc) proved with better accuracies compared with the standard ML models. However, I've not tried any DL models here as per the time constraint as there are other activities to work on as per this assignment.
